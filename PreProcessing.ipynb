{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import ast\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "import string\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_file_path = 'train.csv'\n",
    "test_file_path = 'test.csv'\n",
    "\n",
    "# Read the CSV file into a DataFrame.\n",
    "# If the file does not have headers, specify header=None and assign a column name.\n",
    "df = pd.read_csv(train_file_path, header=None, names=['sentence'])\n",
    "df_test = pd.read_csv(test_file_path, header=None, names=['sentence'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### EDA on train dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DataFrame structure and general info\n",
    "print(\"Training DataFrame Info:\")\n",
    "df.info()\n",
    "\n",
    "# Display summary statistics\n",
    "print(\"\\nDataFrame Summary Statistics:\")\n",
    "print(df.describe(include='all'))\n",
    "\n",
    "# Check the first few rows of the DataFrame\n",
    "print(\"\\nFirst Few Rows:\")\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### EDA on test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DataFrame structure and general info\n",
    "print(\"Testing DataFrame Info:\")\n",
    "df_test.info()\n",
    "\n",
    "# Display summary statistics\n",
    "print(\"\\nDataFrame Summary Statistics:\")\n",
    "print(df_test.describe(include='all'))\n",
    "\n",
    "# Check the first few rows of the DataFrame\n",
    "print(\"\\nFirst Few Rows:\")\n",
    "print(df_test.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### EDA: Comparision between train and test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop Duplicates and Compare\n",
    "print(\"\\nLength of Train Dataset before dropping duplicates:\", len(df))\n",
    "print(\"Length of Test Dataset before dropping duplicates:\", len(df_test))\n",
    "\n",
    "df = df.drop_duplicates(subset=['sentence'])\n",
    "df_test = df_test.drop_duplicates(subset=['sentence'])\n",
    "\n",
    "print(\"\\nLength of Train Dataset after dropping duplicates:\", len(df))\n",
    "print(\"Length of Test Dataset after dropping duplicates:\", len(df_test))\n",
    "\n",
    "unique_to_df1 = df[~df['sentence'].isin(df_test['sentence'])]\n",
    "unique_to_df2 = df_test[~df_test['sentence'].isin(df['sentence'])]\n",
    "common_rows = pd.merge(df, df_test, on='sentence')\n",
    "\n",
    "# Create counts for visualization\n",
    "counts = {\n",
    "    'Unique to Train': len(unique_to_df1),\n",
    "    'Unique to Test': len(unique_to_df2),\n",
    "    'Common Rows': len(common_rows)\n",
    "}\n",
    "\n",
    "print(\"\\nUnique to Train:\", len(unique_to_df1))\n",
    "print(\"Unique to Test:\", len(unique_to_df2))\n",
    "print(\"Common rows:\", len(common_rows))\n",
    "\n",
    "# Plotting comparison for counts\n",
    "plt.bar(counts.keys(), counts.values(), color=['skyblue', 'salmon', 'lightgreen'])\n",
    "plt.title(\"Comparison of Rows Between Test and Train DataFrames (Counts)\")\n",
    "plt.ylabel(\"Number of Rows\")\n",
    "plt.show()\n",
    "\n",
    "# Calculate percentages for better insight\n",
    "total_rows = len(df) + len(df_test)\n",
    "percentages = {key: (value / total_rows) * 100 for key, value in counts.items()}\n",
    "\n",
    "print(\"\\nPercentage of Unique to Train: {:.2f}%\".format(percentages['Unique to Train']))\n",
    "print(\"Percentage of Unique to Test: {:.2f}%\".format(percentages['Unique to Test']))\n",
    "print(\"Percentage of Common Rows: {:.2f}%\".format(percentages['Common Rows']))\n",
    "\n",
    "# Plotting percentage comparison\n",
    "plt.bar(percentages.keys(), percentages.values(), color=['skyblue', 'salmon', 'lightgreen'])\n",
    "plt.title(\"Percentage Comparison Between Test and Train DataFrames\")\n",
    "plt.ylabel(\"Percentage\")\n",
    "plt.show()\n",
    "\n",
    "# Assertions for sanity checks\n",
    "assert len(df) == len(unique_to_df1) + len(common_rows), \"Mismatch in Train dataset counts!\"\n",
    "print(\"Train dataset assertion passed ✅\")\n",
    "\n",
    "assert len(df_test) == len(unique_to_df2) + len(common_rows), \"Mismatch in Test dataset counts!\"\n",
    "print(\"Test dataset assertion passed ✅\")\n",
    "\n",
    "assert sum(counts.values()) == len(df) + len(df_test) - len(common_rows), \"Total row counts assertion passed ✅\"\n",
    "print(\"Total row counts assertion passed ✅\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sentence Parsing\n",
    "Converting the given sentence string to a list of tuples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['parsed_sentence'] = df['sentence'].apply(lambda x: ast.literal_eval(x))\n",
    "df_test['parsed_sentence'] = df_test['sentence'].apply(lambda x: ast.literal_eval(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### EDA on train dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_sentences = len(df)\n",
    "print(\"\\nTotal number of sentences:\", num_sentences)\n",
    "# Display DataFrame information\n",
    "print(\"\\nDataFrame Info:\")\n",
    "print(df.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the first row with all columns\n",
    "print(\"After converting from string to list of tuples, the parsed sentence in first row:\")\n",
    "print(df['parsed_sentence'].iloc[0])\n",
    "print(type(df['parsed_sentence'].iloc[0]))\n",
    "# print more info about the column parsed. like, the datatype and stuff\n",
    "# print(df['parsed'].apply(type).value_counts())\n",
    "print(\"\\nFirst value in first row of the parsed column:\")\n",
    "print(df['parsed_sentence'].iloc[0][0])\n",
    "print(type(df['parsed_sentence'].iloc[0][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate sentence lengths (number of word-tag pairs per sentence)\n",
    "df['sentence_length'] = df['parsed_sentence'].apply(lambda sentence: len(sentence))\n",
    "print(\"\\nSentence length statistics:\")\n",
    "print(df['sentence_length'].describe())\n",
    "\n",
    "print(\"\\nLength of sentences for first few rows:\")\n",
    "print(df['sentence_length'].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the distribution of sentence lengths\n",
    "plt.figure(figsize=(8, 6))\n",
    "df['sentence_length'].hist(bins=20, color='skyblue', edgecolor='black')\n",
    "plt.title(\"Sentence Length Distribution\")\n",
    "plt.xlabel(\"Number of Words per Sentence\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.grid(False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Separating word and corresponding tag pair for each word in all sentences "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list of dictionaries where each dictionary represents a word-tag pair.\n",
    "records = []\n",
    "for sentence in df['parsed_sentence']:\n",
    "    for word, tag in sentence:\n",
    "        records.append({'word': word, 'tag': tag})\n",
    "\n",
    "# Create a new DataFrame from the records.\n",
    "df_words = pd.DataFrame(records)\n",
    "print(df_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### EDA: word-tag distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check basic information and types\n",
    "print(\"Dataset Info:\")\n",
    "df_words.info()\n",
    "\n",
    "# Summary statistics\n",
    "print(\"\\nDataset Summary Statistics:\")\n",
    "print(df_words.describe(include='all'))\n",
    "\n",
    "# Check for missing values\n",
    "print(\"\\nMissing Values:\")\n",
    "print(df_words.isnull().sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the frequency of each tag\n",
    "tag_distribution = df_words['tag'].value_counts()\n",
    "print(\"Length of tag distribution:\", len(tag_distribution))\n",
    "print(\"\\nTag Frequency Distribution:\")\n",
    "print(tag_distribution)\n",
    "\n",
    "# Plot the distribution of tags\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(x=tag_distribution.index, y=tag_distribution.values)\n",
    "plt.title(\"Distribution of Tags\")\n",
    "plt.xlabel(\"POS Tags\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the frequency of words\n",
    "word_frequency = df_words['word'].value_counts()\n",
    "print(\"\\nTop 10 Most Frequent Words:\")\n",
    "print(word_frequency.head(10))\n",
    "\n",
    "# Plot the top 10 most frequent words\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(x=word_frequency.head(10).index, y=word_frequency.head(10).values)\n",
    "plt.title(\"Top 10 Most Frequent Words\")\n",
    "plt.xlabel(\"Words\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step-2. Pre-Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Text Cleaning\n",
    "- Converting to Lowercase\n",
    "- Removing URLs if any using regex library\n",
    "- Removing non-word and non-whitespace characters: punctuations marks, symbols, other special characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_word(word):\n",
    "    # Convert to lowercase\n",
    "    word = word.lower()\n",
    "    \n",
    "    # Remove URLs\n",
    "    word = re.sub(r'http\\S+|www\\S+|https\\S+', '', word)\n",
    "    \n",
    "    # Remove non-word and non-whitespace characters, but not tags like '.'\n",
    "    word = re.sub(r'[^\\w\\s]', '', word)\n",
    "    \n",
    "    # # Remove alphanumeric words containing digits but leave words like underscores\n",
    "    # word = re.sub('\\b\\w*\\d\\w*\\b', '', word)\n",
    "    \n",
    "    # Remove words that consist only of underscores\n",
    "    if re.match(r'^\\_+$', word):\n",
    "        return None  # Remove this word if it consists of only underscores\n",
    "\n",
    "    # Return cleaned word if it's not purely numeric and not empty\n",
    "    return word if word else None\n",
    "\n",
    "def clean_sentence(sentence):\n",
    "    cleaned_sentence = []\n",
    "    for word, tag in sentence:\n",
    "        # Only clean the word, keep the tag unchanged\n",
    "        cleaned_word = clean_word(word)\n",
    "        if cleaned_word and tag != 'X' and tag != 'NUM' and tag != '_' and tag != 'SYM':  # Keep valid cleaned words and remove 'X' tags\n",
    "            cleaned_sentence.append((cleaned_word, tag))\n",
    "    return cleaned_sentence\n",
    "\n",
    "# Apply the updated cleaning function to the parsed sentences\n",
    "df['parsed_sentence'] = df['parsed_sentence'].apply(clean_sentence)\n",
    "# Doing it on test dataset also\n",
    "df_test['parsed_sentence'] = df_test['parsed_sentence'].apply(clean_sentence)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract all words and tags from the parsed sentences\n",
    "all_words = [word for sentence in df['parsed_sentence'] for word, _ in sentence]\n",
    "all_tags = [tag for sentence in df['parsed_sentence'] for __, tag in sentence]\n",
    "print(\"Total words: \", len(all_words), \"Total tags: \", len(all_tags))\n",
    "# Get unique words and tags\n",
    "unique_words = set(all_words)\n",
    "unique_tags = set(all_tags)\n",
    "\n",
    "print(f\"Total Unique Words: {len(unique_words)}\")\n",
    "print(f\"Total Unique Tags: {len(unique_tags)}\")\n",
    "print(\"Few Unique Words:\", list(unique_words)[:10])\n",
    "print(\"\\nFew Unique Tags:\", list(unique_tags))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dictionaries mapping words and tags to indices\n",
    "word_to_index = {word: idx for idx, word in enumerate(sorted(unique_words))}\n",
    "tag_to_index = {tag: idx for idx, tag in enumerate(sorted(unique_tags))}\n",
    "\n",
    "print(\"\\nSample Word to Index Mapping:\", list(word_to_index.items())[:10])\n",
    "print(\"\\nSample Tag to Index Mapping:\", list(tag_to_index.items()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stop word removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Download stopwords if not already downloaded\n",
    "# nltk.download('stopwords')\n",
    "\n",
    "# # Define the stop words set\n",
    "# stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Count the total stop words before removal\n",
    "# total_stop_words_before = sum(\n",
    "#     word.lower() in stop_words for sentence in df['parsed_sentence'] for word, _ in sentence\n",
    "# )\n",
    "\n",
    "# print(f\"Total number of stop words before removal: {total_stop_words_before}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def remove_stop_words(sentence):\n",
    "#     # Remove stop words from the sentence while keeping tags intact\n",
    "#     return [(word, tag) for word, tag in sentence if word.lower() not in stop_words]\n",
    "\n",
    "# # Apply the stop word removal function\n",
    "# df['parsed_sentence'] = df['parsed_sentence'].apply(remove_stop_words)\n",
    "# df_test['parsed_sentence'] = df_test['parsed_sentence'].apply(remove_stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Count the total stop words after removal\n",
    "# total_stop_words_after = sum(\n",
    "#     word.lower() in stop_words for sentence in df['parsed_sentence'] for word, _ in sentence\n",
    "# )\n",
    "\n",
    "# print(f\"Total number of stop words after removal: {total_stop_words_after}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generating Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "# Extract unique tags and words from the parsed sentences to account all the unique tags and words\n",
    "tags = set()\n",
    "words = set()\n",
    "\n",
    "for sentence in df['parsed_sentence']:\n",
    "    for item in sentence:\n",
    "        word, tag = item\n",
    "        tags.add(tag)\n",
    "        words.add(word)\n",
    "\n",
    "tags = list(tags)\n",
    "words = list(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "# Download necessary NLTK data\n",
    "nltk.download(\"averaged_perceptron_tagger\")\n",
    "nltk.download(\"wordnet\")\n",
    "nltk.download(\"omw-1.4\")\n",
    "\n",
    "# Initialize lemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Function to convert NLTK POS tag to WordNet POS tag\n",
    "def get_wordnet_pos(tag):\n",
    "    if tag.startswith('V'):  # Verb\n",
    "        return wordnet.VERB\n",
    "    elif tag.startswith('N'):  # Noun\n",
    "        return wordnet.NOUN\n",
    "    return None  # Ignore other parts of speech\n",
    "\n",
    "# Function to lemmatize words in a sentence based on their POS tags\n",
    "def lemmatize_sentence(sentence):\n",
    "    lemmatized_sentence = []\n",
    "    for word, tag in sentence:\n",
    "        wordnet_pos = get_wordnet_pos(tag)\n",
    "        if wordnet_pos:\n",
    "            lemmatized_word = lemmatizer.lemmatize(word, wordnet_pos)\n",
    "            lemmatized_sentence.append((lemmatized_word, tag))\n",
    "        else:\n",
    "            lemmatized_sentence.append((word, tag))\n",
    "    return lemmatized_sentence\n",
    "\n",
    "# Apply lemmatization to the parsed sentences in the train and test datasets\n",
    "df['parsed_sentence'] = df['parsed_sentence'].apply(lemmatize_sentence)\n",
    "df_test['parsed_sentence'] = df_test['parsed_sentence'].apply(lemmatize_sentence)\n",
    "\n",
    "# Display the first few rows to verify the changes\n",
    "print(\"First few rows of lemmatized train dataset:\")\n",
    "print(df['parsed_sentence'].head())\n",
    "\n",
    "print(\"\\nFirst few rows of lemmatized test dataset:\")\n",
    "print(df_test['parsed_sentence'].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Transition Probabilities\n",
    "  \n",
    "<ul>\n",
    "  <li> Compute <i>P</i>(<i>tag</i><sub>t+1</sub> | <i>tag</i><sub>t</sub>): \n",
    "      the probability of transitioning from one tag to another.\n",
    "  </li>\n",
    "  <li> Formula:\n",
    "      <i>P</i>(<i>tag</i><sub>t+1</sub> | <i>tag</i><sub>t</sub>) =\n",
    "        <span >\n",
    "            Count(<i>tag</i><sub>t+1</sub> given <i>tag</i><sub>t</sub>)\n",
    "        </span>/\n",
    "        Count(<i>tag</i><sub>t</sub>)\n",
    "  </li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute transition probabilities P(tag2 | tag1)\n",
    "transition_counts = defaultdict(lambda: defaultdict(int))\n",
    "tag_counts = defaultdict(int)\n",
    "\n",
    "for sentence in df['parsed_sentence']:\n",
    "    prev_tag = None\n",
    "    for _, tag in sentence:\n",
    "        tag_counts[tag] += 1\n",
    "        if prev_tag is not None:\n",
    "            transition_counts[prev_tag][tag] += 1\n",
    "        prev_tag = tag\n",
    "\n",
    "# doing laplace smoothing to avoid zero probabilities and adding 1 to numerator and len(tags) to denominator for fair probability distribution \n",
    "transition_probs = {\n",
    "    tag1: {tag2: (transition_counts[tag1][tag2] + 1) / (tag_counts[tag1] + len(tags))\n",
    "           for tag2 in tags}\n",
    "    for tag1 in tags\n",
    "}\n",
    "\n",
    "print(\"\\nTransition Probabilities:\\n\", transition_probs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"font-size:16px\">\n",
    "Note:- Why Use Laplacian Smoothing in HMM?\n",
    "<ul style=\"font-size:15px\">\n",
    "    <li> Avoids zero probabilities, preventing the Viterbi algorithm from breaking.\n",
    "    <li> Encourages generalization, allowing the model to predict unseen sequences.\n",
    "    <li> Prevents overfitting, distributing probability mass more fairly among possible transitions.\n",
    "    <li> Handles sparse data, which is common in NLP and speech recognition.\n",
    "</ul>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Emission Probabilities\n",
    "  \n",
    "<ul>\n",
    "  <li> Compute P ( word | tag ) : \n",
    "      <i>the probability of a word being generated by a tag.</i>\n",
    "  </li>\n",
    "  <li> Formula:\n",
    "      <i>P ( word | tag ) = Count(word,tag) / Count(tag)</i>\n",
    "  </li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute emission probabilities P(word | tag)\n",
    "emission_counts = defaultdict(lambda: defaultdict(int))\n",
    "\n",
    "for sentence in df['parsed_sentence']:\n",
    "    for word, tag in sentence:\n",
    "        emission_counts[tag][word] += 1\n",
    "\n",
    "emission_probs = {\n",
    "    tag: {word: (emission_counts[tag][word] + 1) / (tag_counts[tag] + len(words))\n",
    "          for word in words}\n",
    "    for tag in tags\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initial Probabilities\n",
    "  \n",
    "<ul>\n",
    "  <li> Compute <i>P</i>(<i>tag</i><sub>start</sub>)): \n",
    "      the probability of a tag starting the sentence.\n",
    "  </li>\n",
    "  <li> Formula:\n",
    "      <i>P</i>(<i>tag</i><sub>start</sub>) =\n",
    "        <span >\n",
    "            Count(<i>tag</i><sub>start</sub>)\n",
    "        </span>/\n",
    "        Total sentences\n",
    "  </li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute initial probabilities P(tag | start)\n",
    "start_counts = defaultdict(int)\n",
    "total_sentences = len(df['parsed_sentence'])\n",
    "\n",
    "for sentence in df['parsed_sentence']:\n",
    "    if sentence:  # Ensure the sentence is not empty\n",
    "        first_tag = sentence[0][1]\n",
    "        start_counts[first_tag] += 1\n",
    "\n",
    "start_probs = {\n",
    "    tag: (start_counts[tag] + 1) / (total_sentences + len(tags))\n",
    "    for tag in tags\n",
    "}\n",
    "\n",
    "print(\"\\nInitial Probabilities:\\n\", start_probs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Viterbi Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h style=\"font-size:14px\">Now we are using Viterbi Algorithm to find the most likely sequence of hidden states in an HMM.</h>\n",
    "<p style=\"font-size:14px\">\n",
    "    <i>V<sub>t</sub></i>(<i>tag</i>) =\n",
    "        max<sub>prev_tag</sub> [ <i>V<sub>t-1</sub></i>(<i>prev_tag</i>) . P(tag | prev_tag) . P(word | tag) ]\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lexicon = {\n",
    "    'the': 'DET',\n",
    "    'a': 'DET',\n",
    "    'an': 'DET',\n",
    "    'this': 'DET',\n",
    "    'i': 'PRON',\n",
    "    'you': 'PRON',\n",
    "    'they': 'PRON',\n",
    "    'We': 'PRON',\n",
    "    'he': 'PRON',\n",
    "    'she': 'PRON',\n",
    "    'it': 'PRON',\n",
    "    'is': 'AUX',\n",
    "    'was': 'AUX',\n",
    "    'has': 'AUX',\n",
    "    'but': 'CONJ',\n",
    "    'and': 'CONJ',  \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def viterbi_algorithm(sentence, tags, transition_probs, emission_probs, start_probs):\n",
    "    V = [{}]  # Viterbi matrix (list of dictionaries)\n",
    "    backpointer = [{}]  # Backpointer matrix\n",
    "\n",
    "    # **Lexicon-based heuristic for the first word**\n",
    "    first_word = sentence[0].lower()\n",
    "    first_tag = lexicon.get(first_word, None)  # Check if the word exists in the lexicon\n",
    "    \n",
    "    for tag in tags:\n",
    "        if first_tag == tag:  # Give high probability if lexicon match\n",
    "            V[0][tag] = 1.0\n",
    "        else:\n",
    "            V[0][tag] = start_probs.get(tag, 1e-6) * emission_probs[tag].get(first_word, 1e-6)\n",
    "        \n",
    "        backpointer[0][tag] = None\n",
    "    \n",
    "    # # Initialize the first column of the Viterbi matrix\n",
    "    # for tag in tags:\n",
    "    #     V[0][tag] = start_probs.get(tag, 1e-6) * emission_probs[tag].get(sentence[0], 1e-6)\n",
    "    #     backpointer[0][tag] = None\n",
    "    \n",
    "    # Populate the Viterbi matrix\n",
    "    for t in range(1, len(sentence)):\n",
    "        V.append({})\n",
    "        backpointer.append({})\n",
    "        \n",
    "        for tag in tags:\n",
    "            max_prob, best_prev_tag = max(\n",
    "                (V[t-1][prev_tag] * transition_probs[prev_tag].get(tag, 1e-6) * emission_probs[tag].get(sentence[t], 1e-6), prev_tag)\n",
    "                for prev_tag in tags\n",
    "            )\n",
    "            V[t][tag] = max_prob\n",
    "            backpointer[t][tag] = best_prev_tag\n",
    "    \n",
    "    # Backtrack to find the best sequence\n",
    "    best_last_tag = max(V[-1], key=V[-1].get)\n",
    "    best_tags = [best_last_tag]\n",
    "    \n",
    "    for t in range(len(sentence) - 1, 0, -1):\n",
    "        best_tags.insert(0, backpointer[t][best_tags[0]])\n",
    "    \n",
    "    return best_tags"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Testing on sample sentence to check whether our algorithm working properly or not</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample sentence for demonstration\n",
    "sample_sentence = [\"This\", \"is\", \"a\", \"sample\", \"sentence\", \".\"]\n",
    "\n",
    "# Predict tags for the sample sentence using the Viterbi algorithm\n",
    "predicted_tags_sample = viterbi_algorithm(sample_sentence, tags, transition_probs, emission_probs, start_probs)\n",
    "\n",
    "# Print the sample sentence with predicted tags\n",
    "print(\"Sample Sentence:\", sample_sentence)\n",
    "print(\"Predicted Tags:\", predicted_tags_sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluation of the HMM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Applying the trained model on Testing dataset and evaluating the Model by giving the accuracy score and the confusion matrix</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "import numpy as np\n",
    "\n",
    "# Function to predict tags for a given sentence using the Viterbi algorithm\n",
    "def predict_tags(test_sentence, tags, transition_probs, emission_probs, start_probs):\n",
    "    return viterbi_algorithm(test_sentence, tags, transition_probs, emission_probs, start_probs)\n",
    "\n",
    "# Prepare the test data\n",
    "test_sentences = df_test['parsed_sentence'].tolist()\n",
    "true_tags = [[tag for _, tag in sentence] for sentence in test_sentences]\n",
    "\n",
    "# Predict tags for each sentence in the test set\n",
    "predicted_tags = [predict_tags([word for word, _ in sentence], tags, transition_probs, emission_probs, start_probs) if sentence else [] for sentence in test_sentences]\n",
    "\n",
    "# Flatten the lists of true and predicted tags\n",
    "true_tags_flat = [tag for sentence in true_tags for tag in sentence]\n",
    "predicted_tags_flat = [tag for sentence in predicted_tags for tag in sentence]\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(true_tags_flat, predicted_tags_flat)\n",
    "print(f\"Model Accuracy: {accuracy:.3f}\")\n",
    "# Create a DataFrame for incorrectly predicted tags\n",
    "incorrect_predictions = pd.DataFrame({\n",
    "    'Word': [word for sentence in test_sentences for word, _ in sentence],\n",
    "    'True_Tag': true_tags_flat,\n",
    "    'Predicted_Tag': predicted_tags_flat\n",
    "})\n",
    "\n",
    "# Filter out the incorrect predictions\n",
    "incorrect_predictions = incorrect_predictions[incorrect_predictions['True_Tag'] != incorrect_predictions['Predicted_Tag']]\n",
    "\n",
    "# Display the first 10 incorrect predictions\n",
    "print(\"\\nFirst 10 Incorrect Predictions:\")\n",
    "print(incorrect_predictions.head(10))\n",
    "\n",
    "# Display statistics of incorrect predictions\n",
    "print(\"\\nStatistics of Incorrect Predictions:\")\n",
    "print(incorrect_predictions.describe(include='all'))\n",
    "\n",
    "# Generate confusion matrix\n",
    "conf_matrix = confusion_matrix(true_tags_flat, predicted_tags_flat, labels=tags)\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(conf_matrix)\n",
    "\n",
    "# Optionally, generate a classification report\n",
    "class_report = classification_report(true_tags_flat, predicted_tags_flat, labels=tags)\n",
    "print(\"\\nClassification Report:\")\n",
    "print(class_report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plotting the confusion matrix as a heatmap\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(conf_matrix, annot=True, cmap='Blues', fmt='g', xticklabels=tags, yticklabels=tags)\n",
    "plt.title('Confusion Matrix')\n",
    "plt.xlabel('Predicted Tags')\n",
    "plt.ylabel('True Tags')\n",
    "plt.xticks(rotation=45)\n",
    "plt.yticks(rotation=0)\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
